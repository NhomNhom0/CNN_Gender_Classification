{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gender Classification MiniProject\n\nThis mini-project utilizes the pre-trained EfficientNetB3 model to construct a Convolutional Neural Network (CNN) for gender recognition through images. This is an intriguing application of artificial intelligence in classifying and predicting vital information from image data. The objective of this project is to create an automated system capable of accurately determining the gender of individuals in images.\n\n## Step 1: Prepare data\n\n**Data source:** https://www.kaggle.com/datasets/ashishjangra27/gender-recognition-200k-images-celeba\n\n**Data preprocessing:** Convert photos into digital format and re-size it to 64x64 to fit current configuration and model EfficientNetB3. Only use images from the train folder and test folder. I will mix them all and split them again later with my ratio","metadata":{}},{"cell_type":"code","source":"# Import the necessary libraries for data preprocessing\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-11-01T18:58:10.217142Z","iopub.execute_input":"2023-11-01T18:58:10.218240Z","iopub.status.idle":"2023-11-01T18:58:10.229707Z","shell.execute_reply.started":"2023-11-01T18:58:10.218200Z","shell.execute_reply":"2023-11-01T18:58:10.228397Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Preprecessing Female images","metadata":{}},{"cell_type":"code","source":"# Reading image folder path\nfolders = [\n    \"/kaggle/input/gender-recognition-200k-images-celeba/Dataset/Train/Female\",\n    \"/kaggle/input/gender-recognition-200k-images-celeba/Dataset/Test/Female\"\n]\n\nfemale_images_paths = []\n\nfor folder in folders:\n    file_names = os.listdir(folder)\n    sorted_file_names = sorted(file_names, key=lambda x: int(''.join(filter(str.isdigit, x))))\n    image_paths = [os.path.join(folder, filename) for filename in sorted_file_names]\n    female_images_paths.extend(image_paths)\n\nlen(female_images_paths)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T18:58:10.233343Z","iopub.execute_input":"2023-11-01T18:58:10.233680Z","iopub.status.idle":"2023-11-01T18:58:11.971567Z","shell.execute_reply.started":"2023-11-01T18:58:10.233653Z","shell.execute_reply":"2023-11-01T18:58:11.970469Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"104387"},"metadata":{}}]},{"cell_type":"code","source":"# Convert and re-size images\nfemale_data_x = []\n\nfor image_path in female_images_paths:\n    img = Image.open(image_path)\n    img = img.resize((64, 64))\n    img = np.array(img, dtype=\"uint8\")\n    female_data_x.append(img)\n\nprint(\"number of picture:\", len(female_data_x))","metadata":{"execution":{"iopub.status.busy":"2023-11-01T18:58:11.973758Z","iopub.execute_input":"2023-11-01T18:58:11.974493Z","iopub.status.idle":"2023-11-01T19:10:45.211895Z","shell.execute_reply.started":"2023-11-01T18:58:11.974454Z","shell.execute_reply":"2023-11-01T19:10:45.210789Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"number of picture: 104387\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create labels for images (female will be labeled as 1)\nfemale_data_y = np.ones(len(female_data_x))\nlen(female_data_y)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:10:45.213417Z","iopub.execute_input":"2023-11-01T19:10:45.213749Z","iopub.status.idle":"2023-11-01T19:10:45.221291Z","shell.execute_reply.started":"2023-11-01T19:10:45.213720Z","shell.execute_reply":"2023-11-01T19:10:45.220040Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"104387"},"metadata":{}}]},{"cell_type":"markdown","source":"### Preprecessing Male images","metadata":{}},{"cell_type":"code","source":"# Reading image folder path\nfolders = [\n    \"/kaggle/input/gender-recognition-200k-images-celeba/Dataset/Train/Male\",\n    \"/kaggle/input/gender-recognition-200k-images-celeba/Dataset/Test/Male\"\n]\n\nmale_images_paths = []\n\nfor folder in folders:\n    file_names = os.listdir(folder)\n    sorted_file_names = sorted(file_names, key=lambda x: int(''.join(filter(str.isdigit, x))))\n    image_paths = [os.path.join(folder, filename) for filename in sorted_file_names]\n    male_images_paths.extend(image_paths)\n\nlen(male_images_paths)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:10:45.224149Z","iopub.execute_input":"2023-11-01T19:10:45.224544Z","iopub.status.idle":"2023-11-01T19:10:46.683530Z","shell.execute_reply.started":"2023-11-01T19:10:45.224515Z","shell.execute_reply":"2023-11-01T19:10:46.682393Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"75614"},"metadata":{}}]},{"cell_type":"code","source":"# Convert and re-size images\nmale_data_x = []\n\nfor image_path in male_images_paths:\n    img = Image.open(image_path)\n    img = img.resize((64, 64))\n    img = np.array(img, dtype=\"uint8\")\n    male_data_x.append(img)\n\nprint(\"number of picture:\", len(male_data_x))","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:10:46.684964Z","iopub.execute_input":"2023-11-01T19:10:46.685416Z","iopub.status.idle":"2023-11-01T19:19:42.675556Z","shell.execute_reply.started":"2023-11-01T19:10:46.685376Z","shell.execute_reply":"2023-11-01T19:19:42.674453Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"number of picture: 75614\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create labels for images (male will be labeled as 0)\nmale_data_y = np.zeros(len(male_data_x))\nlen(male_data_y)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:19:42.677243Z","iopub.execute_input":"2023-11-01T19:19:42.677665Z","iopub.status.idle":"2023-11-01T19:19:42.685420Z","shell.execute_reply.started":"2023-11-01T19:19:42.677628Z","shell.execute_reply":"2023-11-01T19:19:42.684391Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"75614"},"metadata":{}}]},{"cell_type":"markdown","source":"### Merge data","metadata":{}},{"cell_type":"code","source":"# Merge features data\ndf_x = np.concatenate((female_data_x, male_data_x))\nlen(df_x)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:19:42.686799Z","iopub.execute_input":"2023-11-01T19:19:42.687153Z","iopub.status.idle":"2023-11-01T19:19:44.302932Z","shell.execute_reply.started":"2023-11-01T19:19:42.687121Z","shell.execute_reply":"2023-11-01T19:19:44.301786Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"180001"},"metadata":{}}]},{"cell_type":"code","source":"# Merge labels data\ndf_y = np.concatenate((female_data_y, male_data_y))\nlen(df_y)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:19:44.304303Z","iopub.execute_input":"2023-11-01T19:19:44.304615Z","iopub.status.idle":"2023-11-01T19:19:44.312452Z","shell.execute_reply.started":"2023-11-01T19:19:44.304589Z","shell.execute_reply":"2023-11-01T19:19:44.311331Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"180001"},"metadata":{}}]},{"cell_type":"markdown","source":"### Split data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 90% of the data will be used for training, the remaining 10% will be used for testing\ntrain_data, val_data, train_labels, val_labels = train_test_split(df_x, df_y, test_size=0.1, random_state=42)\n\nprint('lenght train_data:', len(train_data))\nprint('lenght val_data:', len(val_data))\nprint('lenght train_labels:', len(train_labels))\nprint('lenght val_labels:', len(val_labels))","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:19:44.314171Z","iopub.execute_input":"2023-11-01T19:19:44.314902Z","iopub.status.idle":"2023-11-01T19:19:46.131234Z","shell.execute_reply.started":"2023-11-01T19:19:44.314862Z","shell.execute_reply":"2023-11-01T19:19:46.130126Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Step 2: Training model\n\n**Pre-trained model:** EfficientNetB3\n\n**Optimization algorithm:** Adam\n\n**Epoch:** 5","metadata":{}},{"cell_type":"code","source":"# Run this if this is the first time run this notebook\n!pip install tensorflow==2.9","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:19:46.135220Z","iopub.execute_input":"2023-11-01T19:19:46.135734Z","iopub.status.idle":"2023-11-01T19:20:58.789618Z","shell.execute_reply.started":"2023-11-01T19:19:46.135691Z","shell.execute_reply":"2023-11-01T19:20:58.788493Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Collecting tensorflow==2.9\n  Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.6.3)\nCollecting flatbuffers<2,>=1.12 (from tensorflow==2.9)\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (3.9.0)\nCollecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9)\n  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.9)\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (16.0.0)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (21.3)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (68.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.16.0)\nCollecting tensorboard<2.10,>=2.9 (from tensorflow==2.9)\n  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (0.32.0)\nCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9)\n  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (4.6.3)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.14.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.9) (0.40.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (2.20.0)\nCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9)\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (3.4.3)\nCollecting protobuf>=3.9.2 (from tensorflow==2.9)\n  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (2.31.0)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9)\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9)\n  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (2.3.7)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.9) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.2.2)\nInstalling collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n  Attempting uninstall: keras\n    Found existing installation: keras 2.12.0\n    Uninstalling keras-2.12.0:\n      Successfully uninstalled keras-2.12.0\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 23.5.26\n    Uninstalling flatbuffers-23.5.26:\n      Successfully uninstalled flatbuffers-23.5.26\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.12.0\n    Uninstalling tensorflow-estimator-2.12.0:\n      Successfully uninstalled tensorflow-estimator-2.12.0\n  Attempting uninstall: tensorboard-data-server\n    Found existing installation: tensorboard-data-server 0.7.1\n    Uninstalling tensorboard-data-server-0.7.1:\n      Successfully uninstalled tensorboard-data-server-0.7.1\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: google-auth-oauthlib\n    Found existing installation: google-auth-oauthlib 1.0.0\n    Uninstalling google-auth-oauthlib-1.0.0:\n      Successfully uninstalled google-auth-oauthlib-1.0.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.12.3\n    Uninstalling tensorboard-2.12.3:\n      Successfully uninstalled tensorboard-2.12.3\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.12.0\n    Uninstalling tensorflow-2.12.0:\n      Successfully uninstalled tensorflow-2.12.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.19.6 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.9.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-pubsub 2.17.1 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nonnx 1.14.1 requires protobuf>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-decision-forests 1.4.0 requires tensorflow~=2.12.0, but you have tensorflow 2.9.0 which is incompatible.\ntensorflow-serving-api 2.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-serving-api 2.12.1 requires tensorflow<3,>=2.12.0, but you have tensorflow 2.9.0 which is incompatible.\ntensorflow-text 2.12.1 requires tensorflow<2.13,>=2.12.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed flatbuffers-1.12 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import the necessary libraries for training model\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.keras.optimizers.legacy import Adam\nfrom tensorflow.keras.optimizers import RMSprop\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing import image\nimport tensorflow.keras.optimizers as optimizers\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:20:58.791364Z","iopub.execute_input":"2023-11-01T19:20:58.791822Z","iopub.status.idle":"2023-11-01T19:21:03.824817Z","shell.execute_reply.started":"2023-11-01T19:20:58.791777Z","shell.execute_reply":"2023-11-01T19:21:03.823592Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### ImageDataGenerator to augment data for training","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=False,\n    fill_mode='nearest'\n)\n\nbatch_size = 512\naugmented_data_generator = datagen.flow(train_data, train_labels, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:21:03.826344Z","iopub.execute_input":"2023-11-01T19:21:03.827031Z","iopub.status.idle":"2023-11-01T19:21:05.825330Z","shell.execute_reply.started":"2023-11-01T19:21:03.826996Z","shell.execute_reply":"2023-11-01T19:21:05.824272Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Setup to save the model with the best results","metadata":{}},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\n    'best_model_checkpoint.h5',\n    monitor='val_accuracy',\n    save_best_only=True,\n    mode='max',\n    verbose=1\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:21:05.826917Z","iopub.execute_input":"2023-11-01T19:21:05.827258Z","iopub.status.idle":"2023-11-01T19:21:05.831907Z","shell.execute_reply.started":"2023-11-01T19:21:05.827223Z","shell.execute_reply":"2023-11-01T19:21:05.830921Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Build structure for the model","metadata":{}},{"cell_type":"code","source":"pre_model = models.Sequential()\n\nbase_model = tf.keras.applications.EfficientNetB3(\n              include_top= False,\n              weights=\"imagenet\",\n)\n\n\nfor layer in base_model.layers:\n    layer.trainable = True\n\npre_model.add(base_model)\npre_model.add(layers.GlobalAveragePooling2D())\npre_model.add(layers.Flatten())\npre_model.add(layers.Dense(256, activation='relu'))\npre_model.add(layers.Dropout(0.2))\npre_model.add(layers.Dense(128, activation='relu'))\npre_model.add(layers.Dropout(0.2))\npre_model.add(layers.Dense(1, activation='sigmoid'))\n\npre_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:21:05.833206Z","iopub.execute_input":"2023-11-01T19:21:05.833564Z","iopub.status.idle":"2023-11-01T19:21:13.369584Z","shell.execute_reply.started":"2023-11-01T19:21:05.833536Z","shell.execute_reply":"2023-11-01T19:21:13.368492Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n43941136/43941136 [==============================] - 0s 0us/step\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n efficientnetb3 (Functional)  (None, None, None, 1536)  10783535 \n                                                                 \n global_average_pooling2d (G  (None, 1536)             0         \n lobalAveragePooling2D)                                          \n                                                                 \n flatten (Flatten)           (None, 1536)              0         \n                                                                 \n dense (Dense)               (None, 256)               393472    \n                                                                 \n dropout (Dropout)           (None, 256)               0         \n                                                                 \n dense_1 (Dense)             (None, 128)               32896     \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_2 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 11,210,032\nTrainable params: 11,122,729\nNon-trainable params: 87,303\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Fitting data for the model for training","metadata":{}},{"cell_type":"code","source":"pre_model.compile(optimizer= Adam(learning_rate=0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])\nhistory = pre_model.fit(\n    augmented_data_generator,\n    validation_data=(val_data, val_labels),\n    epochs=5,\n    batch_size=512,\n    callbacks=[checkpoint]\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:21:13.371007Z","iopub.execute_input":"2023-11-01T19:21:13.371390Z","iopub.status.idle":"2023-11-01T19:40:41.641887Z","shell.execute_reply.started":"2023-11-01T19:21:13.371360Z","shell.execute_reply":"2023-11-01T19:40:41.640682Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/5\n317/317 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.8511\nEpoch 1: val_accuracy improved from -inf to 0.93456, saving model to best_model_checkpoint.h5\n317/317 [==============================] - 262s 739ms/step - loss: 0.3229 - accuracy: 0.8511 - val_loss: 0.1728 - val_accuracy: 0.9346\nEpoch 2/5\n317/317 [==============================] - ETA: 0s - loss: 0.1685 - accuracy: 0.9322\nEpoch 2: val_accuracy improved from 0.93456 to 0.94950, saving model to best_model_checkpoint.h5\n317/317 [==============================] - 225s 710ms/step - loss: 0.1685 - accuracy: 0.9322 - val_loss: 0.1321 - val_accuracy: 0.9495\nEpoch 3/5\n317/317 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9497\nEpoch 3: val_accuracy improved from 0.94950 to 0.95672, saving model to best_model_checkpoint.h5\n317/317 [==============================] - 225s 709ms/step - loss: 0.1291 - accuracy: 0.9497 - val_loss: 0.1110 - val_accuracy: 0.9567\nEpoch 4/5\n317/317 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9578\nEpoch 4: val_accuracy improved from 0.95672 to 0.96078, saving model to best_model_checkpoint.h5\n317/317 [==============================] - 225s 709ms/step - loss: 0.1089 - accuracy: 0.9578 - val_loss: 0.1002 - val_accuracy: 0.9608\nEpoch 5/5\n317/317 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9624\nEpoch 5: val_accuracy improved from 0.96078 to 0.96506, saving model to best_model_checkpoint.h5\n317/317 [==============================] - 229s 723ms/step - loss: 0.0984 - accuracy: 0.9624 - val_loss: 0.0913 - val_accuracy: 0.9651\n","output_type":"stream"}]}]}